{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86e4ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"thedevastator/nlp-mental-health-conversations\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebb09c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(path+\"/train.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1734e3b",
   "metadata": {
    "id": "a1734e3b"
   },
   "source": [
    "## Step 1: Basic EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06deae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()  ## Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb123a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['Response'].isnull()]['Response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22702f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop the rows with missing information\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "data.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ede170",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d550475a",
   "metadata": {
    "id": "d550475a"
   },
   "source": [
    "## Step 2: Clean the text (Data Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134ae56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Response'].count()  ## After removal of missing rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45db4a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = data[data['Response'].duplicated(keep=False)]['Response']  ## Finding the duplicates\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c02ff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40d3729",
   "metadata": {
    "id": "c40d3729"
   },
   "source": [
    "Note: This first selects all occurrences of duplicate values.\n",
    "\n",
    "Then:\n",
    "\n",
    "False = first occurrence of each duplicated value\n",
    "\n",
    "True = every later occurrence of a duplicate value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0020a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Other way to check Unique values\n",
    "\n",
    "unique_txt = data['Response'].value_counts()[lambda x: x > 1].index.tolist()\n",
    "len(unique_txt)  ## Unique value count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35b12a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code to count the number of punctuation marks\n",
    "\n",
    "import string\n",
    "from collections import Counter\n",
    "from typing import List, Dict\n",
    "\n",
    "def count_punctuation_in_list(text_list: List[str]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Analyzes a list of text strings (e.g., cleaned 'Response' data)\n",
    "    to count the occurrences of each unique punctuation mark.\n",
    "\n",
    "    Args:\n",
    "        text_list: A list of strings (your DataFrame column converted to a list).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are punctuation marks (e.g., '.', ',') and\n",
    "        values are their total counts across all texts.\n",
    "    \"\"\"\n",
    "    # Use collections.Counter for efficient counting\n",
    "    punctuation_counts = Counter()\n",
    "\n",
    "    # Define the set of punctuation characters to track\n",
    "    # string.punctuation includes: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
    "    all_punctuation = string.punctuation\n",
    "\n",
    "    # Iterate through each text and count punctuation\n",
    "    for text in text_list:\n",
    "        if isinstance(text, str):\n",
    "            for char in text:\n",
    "                if char in all_punctuation:\n",
    "                    punctuation_counts[char] += 1\n",
    "\n",
    "    # Convert the Counter object to a standard dictionary for return\n",
    "    return dict(punctuation_counts)\n",
    "\n",
    "\n",
    "counts = count_punctuation_in_list(unique_txt)\n",
    "\n",
    "print(\"--- Punctuation Count Results (for EDA) ---\")\n",
    "print(f\"Total Unique Punctuation Marks Found: {len(counts)}\")\n",
    "print(\"\\nIndividual Counts:\")\n",
    "# Print the results sorted by count (most frequent first)\n",
    "for char, count in sorted(counts.items(), key=lambda item: item[1], reverse=True):\n",
    "    print(f\"'{char}': {count}\")\n",
    "\n",
    "# --- EDA Insight ---\n",
    "# For your assignment, you would use this data to calculate the percentage of\n",
    "# each punctuation mark relative to the total number of words (for class imbalance analysis).\n",
    "total_words = sum(len(s.split()) for s in unique_txt)\n",
    "print(f\"\\nTotal words in example data: {total_words}\")\n",
    "print(f\"Percentage of words followed by a PERIOD (.): {counts.get('.', 0) / total_words * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7f22e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(u_text):\n",
    "    for i, txt in enumerate(u_text):\n",
    "        txt = txt.strip()\n",
    "\n",
    "        # Collapse multiple spaces\n",
    "        txt = re.sub(r'\\s+', ' ', txt)\n",
    "\n",
    "        # Remove basic HTML entities (e.g., &amp;, &gt;)\n",
    "        txt = re.sub(r'&[a-z]+;', '', txt)\n",
    "\n",
    "        # Replace curly quotes with straight quotes\n",
    "        txt = re.sub(r'[\\u201c\\u201d]', '\"', txt)\n",
    "        txt = re.sub(r'[\\u2018\\u2019]', \"'\", txt)\n",
    "\n",
    "        # Replace multiple periods with ellipsis\n",
    "        txt = re.sub(r'\\.{2,}', '...', txt)\n",
    "\n",
    "        u_text[i] = txt\n",
    "\n",
    "    return u_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285fe407",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_unique_txt=clean_text(unique_txt)\n",
    "clean_unique_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565213a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Response'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d894fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Response'] = data['Response'].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c68ea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Response'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ade915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the spaces\n",
    "\n",
    "data['Response'] = data['Response'].str.strip()\n",
    "data['Response'] = data['Response'].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "# Remove basic HTML entities or common artifacts (e.g., &amp;, &gt;)\n",
    "data['Response'] = data['Response'].str.replace(r'&[a-z]+;', '', regex=True)\n",
    "\n",
    "# Replace curly quotes with straight quotes\n",
    "data['Response'] = data['Response'].str.replace(r'[\\u201c\\u201d]', '\"', regex=True)\n",
    "data['Response'] = data['Response'].str.replace(r'[\\u2018\\u2019]', \"'\", regex=True)\n",
    "\n",
    "# Replace multiple periods (ellipsis) with three periods to standardize\n",
    "data['Response'] = data['Response'].str.replace(r'\\.{2,}', '...', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f357416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the text\n",
    "\n",
    "def normalize_text(text):\n",
    "    # replace punctuation with \" <punct> \" so spacing is preserved\n",
    "    text = re.sub(r\"([.,!?;:])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # normalize spaces\n",
    "    # remove punctuation\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    return text.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c653ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data=data['Response']\n",
    "final_data.dropna(inplace=True)\n",
    "final_data.reset_index(drop=True,inplace=True)\n",
    "final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9074412b",
   "metadata": {
    "id": "9074412b"
   },
   "source": [
    "## Step 3: Creating Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5047872",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"input_text\": final_data.apply(normalize_text),   # text without punctuation\n",
    "    \"output_text\": final_data                             # original text with punctuation\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80b336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"input_text\"] != df[\"output_text\"]]\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd98c27",
   "metadata": {
    "id": "7bd98c27"
   },
   "source": [
    "## Step 4: Build Sequence Labeling Dataset for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab48022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "# import re\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Any, Union\n",
    "\n",
    "# --- Configuration ---\n",
    "# 0 is the ID for the 'NONE' label (no punctuation)\n",
    "PUNCT_MAP = {'.': 1, ',': 2, '?': 3, '!': 4, ':': 5, ';': 6}\n",
    "IGNORE_IDX = -100\n",
    "BERT_MODEL = 'bert-base-uncased'\n",
    "\n",
    "# Initialize the tokenizer globally\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124b0429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(punctuated_text: str, tokenized_input: Dict[str, Any]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Creates the final numerical labels aligned with BERT tokens.\n",
    "    This function relies on the fully punctuated text (the ground truth)\n",
    "    to extract the punctuation associated with each word.\n",
    "\n",
    "    Args:\n",
    "        punctuated_text: The fully punctuated ground-truth sentence (e.g., from 'Response').\n",
    "        tokenized_input: The output dictionary from the BERT tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        A list of integer label IDs, aligned to the BERT tokens.\n",
    "    \"\"\"\n",
    "    # 1. Clean and split the ground truth text into words (including trailing punctuation)\n",
    "    # We use regex to preserve contractions but split punctuation from words (e.g., [\"boy\", \".\"])\n",
    "    # However, since we rely on word_ids(), a simple split and re.sub is more reliable here.\n",
    "\n",
    "    # Simple split of the ground truth text\n",
    "    # Note: If your original data cleaning step was imperfect, this might introduce errors.\n",
    "    words_with_punct = re.findall(r\"[\\w']+|[.,?!:;]\", punctuated_text.lower())\n",
    "\n",
    "    # Filter out remaining single-punctuation tokens that are not needed as labels\n",
    "    # We only care about the punctuation if it's ATTACHED to a word.\n",
    "\n",
    "    # 2. Extract words and their trailing punctuation label\n",
    "    word_labels = [] # Stores (word_string_cleaned, label_id)\n",
    "    current_word = []\n",
    "\n",
    "    # Iterate through the pre-split tokens to pair words with their trailing punctuation\n",
    "    i = 0\n",
    "    while i < len(words_with_punct):\n",
    "        token = words_with_punct[i]\n",
    "\n",
    "        # Check if the token is a punctuation mark we care about\n",
    "        if token in PUNCT_MAP:\n",
    "            # Punctuation follows the previous word\n",
    "            if current_word:\n",
    "                word_labels.append((\" \".join(current_word), PUNCT_MAP[token]))\n",
    "                current_word = []\n",
    "            # If punctuation starts the sentence (should be cleaned already, but safer to skip)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        # If the token is a word\n",
    "        else:\n",
    "            # If current_word is not empty, the previous word ended without a target punctuation mark\n",
    "            if current_word:\n",
    "                word_labels.append((\" \".join(current_word), 0)) # Label 0 = NONE\n",
    "                current_word = [] # Reset for the new word\n",
    "\n",
    "            current_word.append(token)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # Handle the very last word if it was not followed by punctuation\n",
    "    if current_word:\n",
    "        word_labels.append((\" \".join(current_word), 0)) # Label 0 = NONE\n",
    "\n",
    "\n",
    "    # --- 3. Align Labels to BERT Tokens using word_ids ---\n",
    "\n",
    "    # The word_ids() map returns a list of indices where each index corresponds\n",
    "    # to the original word list (word_labels). None indicates special tokens.\n",
    "    word_ids = tokenized_input.word_ids()\n",
    "    labels = []\n",
    "    previous_word_idx = None\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "        # a) Handle Special Tokens and Padding\n",
    "        if word_idx is None:\n",
    "            labels.append(IGNORE_IDX)\n",
    "\n",
    "        # b) Handle First Sub-word Piece of a Word\n",
    "        elif word_idx != previous_word_idx:\n",
    "            # Use the index from word_ids to look up the label from our pre-processed list\n",
    "            if word_idx < len(word_labels):\n",
    "                # The label is the second element of the tuple (word_string_cleaned, label_id)\n",
    "                label_id = word_labels[word_idx][1]\n",
    "                labels.append(label_id)\n",
    "            else:\n",
    "                # Should not happen if data is clean, but assign NONE as fallback\n",
    "                labels.append(0)\n",
    "\n",
    "        # c) Handle Subsequent Sub-word Pieces (e.g., '##ing')\n",
    "        else:\n",
    "            labels.append(IGNORE_IDX)\n",
    "\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13f6739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df: pd.DataFrame, max_length: int = 128) -> Dict[str, List[List[int]]]:\n",
    "    \"\"\"\n",
    "    Prepares the entire DataFrame for BERT Sequence Labeling training.\n",
    "\n",
    "    NOTE: The DataFrame 'df' MUST have two columns:\n",
    "          - 'input_text': The unpunctuated text (for tokenization).\n",
    "          - 'output_text': The fully punctuated ground truth text (for label extraction).\n",
    "    \"\"\"\n",
    "    input_ids, attention_masks, labels_list = [], [], []\n",
    "\n",
    "    # Use tqdm for progress tracking in a real environment\n",
    "    for index, row in df.iterrows():\n",
    "        # 1. Tokenize the UNPUNCTUATED input text\n",
    "        # BERT handles the tokenization and adds CLS/SEP tokens automatically\n",
    "        tokenized_input = tokenizer(\n",
    "            row['input_text'],\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=None, # Ensure non-tensor output for list processing\n",
    "            is_split_into_words=False # We feed strings, BERT handles the split\n",
    "        )\n",
    "\n",
    "        # 2. Create the labels using the fully PUNCTUATED output text\n",
    "        # This function performs the alignment\n",
    "        labels = create_labels(row['output_text'], tokenized_input)\n",
    "\n",
    "        # Ensure labels length matches input_ids length (critical)\n",
    "        if len(tokenized_input['input_ids']) == len(labels):\n",
    "            input_ids.append(tokenized_input['input_ids'])\n",
    "            attention_masks.append(tokenized_input['attention_mask'])\n",
    "            labels_list.append(labels)\n",
    "        else:\n",
    "            # Log or handle sentences where the token/label alignment failed\n",
    "            print(f\"Skipping sentence {index}: Label length ({len(labels)}) does not match token length ({len(tokenized_input['input_ids'])}).\")\n",
    "\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_masks, 'labels': labels_list}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eb0ddd",
   "metadata": {
    "id": "d5eb0ddd"
   },
   "source": [
    "## Step 5: Prepare the train-val-test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4109b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_processed_data_fixed(processed_data, train_size=0.8, val_size=0.1, test_size=0.1):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import numpy as np\n",
    "    if not np.isclose(train_size + val_size + test_size, 1.0):\n",
    "        raise ValueError(\"Sizes must sum to 1.0\")\n",
    "\n",
    "    n = len(processed_data['input_ids'])\n",
    "    indices = np.arange(n)\n",
    "\n",
    "    temp_size = val_size + test_size\n",
    "    train_idx, temp_idx = train_test_split(indices, test_size=temp_size, random_state=42)\n",
    "    val_idx, test_idx = train_test_split(temp_idx, test_size=test_size/(val_size+test_size), random_state=42)\n",
    "\n",
    "    return {\n",
    "        'train': {k: [v[i] for i in train_idx] for k, v in processed_data.items()},\n",
    "        'validation': {k: [v[i] for i in val_idx] for k, v in processed_data.items()},\n",
    "        'test': {k: [v[i] for i in test_idx] for k, v in processed_data.items()}\n",
    "    }\n",
    "\n",
    "processed_data = prepare_dataset(df,max_length=476)\n",
    "splits = split_processed_data_fixed(processed_data)\n",
    "print(f\"Training: {len(splits['train']['input_ids'])}\")\n",
    "print(f\"Validation: {len(splits['validation']['input_ids'])}\")\n",
    "print(f\"Test: {len(splits['test']['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b2782a",
   "metadata": {
    "id": "12b2782a"
   },
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35232e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertForTokenClassification, TrainingArguments, Trainer, AutoTokenizer, DataCollatorForTokenClassification\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# --- Configuration (MUST MATCH data_processor.py) ---\n",
    "BERT_MODEL = 'bert-base-uncased'\n",
    "NUM_LABELS = 7 # NONE(0), PERIOD(1), COMMA(2), QUESTION_MARK(3), EXCLAMATION_MARK(4), COLON(5), SEMICOLON(6)\n",
    "IGNORE_IDX = -100 # Used for [CLS], [SEP], [PAD], and sub-word tokens\n",
    "#MAX_LENGTH=476 determined from your EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b8f0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Custom Dataset Class ---\n",
    "class PunctuationDataset(Dataset):\n",
    "    \"\"\"A custom dataset class for PyTorch, compatible with Hugging Face Trainer.\"\"\"\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert list of lists to PyTorch tensors for a single sample\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self,):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "# --- Step 2: Compute Class Weights for Loss Function ---\n",
    "def compute_class_weights(train_labels: List[List[int]]) -> torch.Tensor:\n",
    "    \"\"\"Calculates inverse frequency weights for handling class imbalance.\"\"\"\n",
    "    \n",
    "    # Flatten the labels and remove IGNORE_IDX\n",
    "    flat_labels = [label for sublist in train_labels for label in sublist if label != IGNORE_IDX]\n",
    "    \n",
    "    # Count frequency of each label\n",
    "    label_counts = Counter(flat_labels)\n",
    "    \n",
    "    # Ensure all labels up to NUM_LABELS are present (even if count is 0)\n",
    "    for i in range(NUM_LABELS):\n",
    "        if i not in label_counts:\n",
    "            label_counts[i] = 1 # Assign 1 to prevent division by zero, though unlikely\n",
    "            \n",
    "    total_samples = len(flat_labels)\n",
    "    \n",
    "    \n",
    "    # Calculate weights: Inverse frequency\n",
    "    # Weight_i = total_samples / (NUM_LABELS * count_i)\n",
    "    weights = []\n",
    "    for i in range(NUM_LABELS):\n",
    "        # Standard inverse frequency scaling\n",
    "        weight = total_samples / (NUM_LABELS * label_counts[i]) \n",
    "        weights.append(weight)\n",
    "\n",
    "    print(\"\\n--- Class Weight Calculation ---\")\n",
    "    print(\"Label Frequencies:\", {i: label_counts[i] for i in range(NUM_LABELS)})\n",
    "    print(\"Calculated Weights (for PyTorch CrossEntropyLoss):\")\n",
    "    \n",
    "    class_weights = torch.tensor(weights, dtype=torch.float32)\n",
    "    print(class_weights)\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac95e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Custom Trainer for Weighted Loss ---\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    \"\"\"Custom Trainer to override the default loss calculation with class weights.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Store class_weights on CPU initially. It will be moved to the correct device during compute_loss.\n",
    "        self.class_weights_cpu = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"Custom loss calculation using weighted Cross-Entropy.\"\"\"\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "\n",
    "        # Move class_weights to the same device as the logits right before use\n",
    "        class_weights_on_device = self.class_weights_cpu.to(logits.device)\n",
    "\n",
    "        # PyTorch CrossEntropyLoss automatically handles the IGNORE_IDX=-100\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_on_device, ignore_index=IGNORE_IDX)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# --- Step 4: Metric Calculation Function (Macro F1 & Accuracy) ---\n",
    "def compute_metrics(p: Any) -> Dict[str, float]:\n",
    "    \"\"\"Calculates the Macro F1 Score and Accuracy for evaluation.\"\"\"\n",
    "    predictions, labels = p.predictions, p.label_ids\n",
    "\n",
    "    # Get the index of the highest logit (the predicted label ID)\n",
    "    predicted_label_ids = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Flatten arrays and mask out the IGNORE_IDX\n",
    "    true_labels = labels.flatten()\n",
    "    pred_labels = predicted_label_ids.flatten()\n",
    "\n",
    "    mask = true_labels != IGNORE_IDX\n",
    "\n",
    "    filtered_true = true_labels[mask]\n",
    "    filtered_pred = pred_labels[mask]\n",
    "\n",
    "    # Calculate Macro F1 Score (Primary Metric)\n",
    "    # This treats all classes equally, penalizing the model for missing rare punctuation.\n",
    "    macro_f1 = f1_score(filtered_true, filtered_pred, average='macro', zero_division=0)\n",
    "\n",
    "    # Calculate Accuracy (Secondary Metric)\n",
    "    accuracy = accuracy_score(filtered_true, filtered_pred)\n",
    "\n",
    "    # Perform EDA on Test Results: Calculate Confusion Matrix (Optional but highly recommended)\n",
    "    # cm = confusion_matrix(filtered_true, filtered_pred, labels=list(range(NUM_LABELS)))\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"macro_f1\": macro_f1\n",
    "        # \"confusion_matrix\": cm # If you want to log the matrix\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1090e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_setup(splits: Dict[str, Dict[str, List[List[int]]]]):\n",
    "    \"\"\"Initializes and runs the training setup.\"\"\"\n",
    "\n",
    "    # 1. Prepare Datasets\n",
    "    train_dataset = PunctuationDataset(splits['train'])\n",
    "    val_dataset = PunctuationDataset(splits['validation'])\n",
    "    test_dataset = PunctuationDataset(splits['test']) # Used for final evaluation\n",
    "\n",
    "    # 2. Compute Weights\n",
    "    class_weights = compute_class_weights(splits['train']['labels'])\n",
    "\n",
    "    # 3. Load Model\n",
    "    # Configure the model to know the number of output labels\n",
    "    model = BertForTokenClassification.from_pretrained(\n",
    "        BERT_MODEL,\n",
    "        num_labels=NUM_LABELS\n",
    "    )\n",
    "\n",
    "    # 4. Define Data Collator\n",
    "    # This collator will handle padding sequences to the same length within each batch\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding='longest')\n",
    "\n",
    "\n",
    "    # 5. Define Training Arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=5,                     # Recommended initial setting for fine-tuning\n",
    "        per_device_train_batch_size=8,          # Adjust based on your environment's GPU memory\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_steps=500,                       # Number of steps for learning rate warmup\n",
    "        weight_decay=0.01,                      # L2 regularization\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=100,\n",
    "        eval_strategy=\"epoch\",            # Evaluate after each epoch\n",
    "        save_strategy=\"epoch\",                  # Save checkpoint after each epoch\n",
    "        load_best_model_at_end=True,            # Load the model with the best validation performance\n",
    "    )\n",
    "\n",
    "    # 6. Initialize Trainer with Weighted Loss and Data Collator\n",
    "    trainer = WeightedLossTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        class_weights=class_weights.to(model.device), # Ensure weights are on the model's device\n",
    "        data_collator=data_collator, # Pass the data collator here\n",
    "    )\n",
    "\n",
    "    # 7. Train the Model (Fine-tuning on Domain Data)\n",
    "    print(\"\\n--- Starting Model Fine-Tuning ---\")\n",
    "    trainer.train()\n",
    "\n",
    "    # 8. Final Evaluation on Test Set (for Assignment Report)\n",
    "    print(\"\\n--- Evaluating Fine-Tuned Model on Test Set ---\")\n",
    "    results = trainer.evaluate(test_dataset)\n",
    "    print(\"Fine-Tuned Model Results:\", results)\n",
    "\n",
    "    # --- For Comparison: Baseline Evaluation ---\n",
    "    # Load the model WITHOUT fine-tuning (Baseline)\n",
    "    baseline_model = BertForTokenClassification.from_pretrained(BERT_MODEL, num_labels=NUM_LABELS)\n",
    "    baseline_trainer = Trainer(\n",
    "        model=baseline_model,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=data_collator, # Pass the data collator here as well\n",
    "        eval_dataset=val_dataset, # Added eval_dataset for baseline_trainer\n",
    "    )\n",
    "    print(\"\\n--- Evaluating Pre-Trained Baseline Model on Test Set ---\")\n",
    "    baseline_results = baseline_trainer.evaluate(test_dataset)\n",
    "    print(\"Baseline Model Results:\", baseline_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b8a690",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training_setup(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8OBgpJfNWuuZ",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
